## <center>在scrapy框架中数据存储</center>

### 一、写入到本地文件中

* 1、方法一、直接在运行命令的时候生成文件

  ```py
  scrapy crawl 爬虫名字 -o 文件名
  ```

* 2、方式二、使用`json`直接写入

  ```py
  import json

  class JsonPipeline(object):
      def __init__(self):
          self.file = open('blog.json', 'a', encoding='utf8')

      def process_item(self, item, spider):
          if spider.name == 'blog':
              self.file.write(json.dumps(dict(item), indent=2, ensure_ascii=False) + ',\n')
          return item

      def close_spider(self, spider):
          self.file.close()
  ```

* 3、方式三、使用`codecs`模块写入

  ```py
  import json  # 记得添加这两个库
  import codecs

  class ScrapytestPipeline(object):
      def __init__(self):
          self.file = codecs.open('item.json', 'wb', encoding='utf-8')

      def process_item(self, item, spider):
          line = json.dumps(dict(item), indent=2, ensure_ascii=False) + ',\n'
          self.file.write(line)  # 写入到文件中
          return item

      def close_spider(self, spider):
          if spider.name == 'xx':
              self.file.close()
  ```

### 二、写入到`mongodb`数据库中(参考`pipelines`这章节)
### 三、写入到`mysql`数据库中
