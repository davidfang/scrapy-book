### <center>scrapy项目基本配置及常用命令</center>

### 一、基本配置(全部在`settings.py`文件中配置)

* 1、是否遵守机器人协议

  ```py
  ROBOTSTXT_OBEY = False
  ```

* 2、配置编码

  ```py
  # 设置编码
  FEED_EXPORT_ENCODING = 'utf-8'
  ```

* 3、设置下载延时(类似前面的休眠多少时间再爬取)

  ```py
  DOWNLOAD_DELAY = 3
  ```

* 4、设置请求头(不设置是默认的`python`的请求头)

  ```py
  # 模拟浏览器请求
  DEFAULT_REQUEST_HEADERS = {
      'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/65.0.3325.162 Safari/537.36'
  }
  ```

### 二、常见的命令

* 1、创建项目

  ```py
  scrapy statrproject 项目名称
  ```

* 2、创建一个普通爬虫

  ```py
  scrapy genapider 爬虫名称 网址
  ```

* 3、创建一个高级爬虫(下面章节会介绍)

  ```py
  scrapy genspider -t crawl 爬虫名字 允许访问的网址
  ```

* 4、查看可用的爬虫模板

  ```py
  scrapy genspider -l
  ```

* 5、启动爬虫

  ```py
  scrapy crawl quotes # 直接运行
  scrapy crawl quotes -o quotes.json # 生成json文件存到本地
  scrapy crawl quotes -o quotes.json --nolog # 启动爬虫不带日志
  ```

* 6、检查爬虫

  ```py
  scrapy check 爬虫名
  ```

* 7、查看本工程下有多少爬虫

  ```py
  scrapy list
  ```

### 三、关于`scrapy shell`的使用

* 1、关于`scrapy shell`的介绍

  我们想要在爬虫中使用`xpath`、`beautifulsoup`、正则表达式、`css`选择器等来提取想要的数据。但是因为`scrapy`是一个比较重的框架。每次运行起来都要等待一段时间。因此要去验证我们写的提取规则是否正确，是一个比较麻烦的事情。因此`Scrapy`提供了一个`shell`，用来方便的测试规则。

* 2、打开`scrapy shell`窗口

  打开`cmd`终端，进入到`Scrapy`项目所在的目录，然后进入到scrapy框架所在的虚拟环境中，输入命令<font color="#f00">`scrapy shell [链接]`</font>。就会进入到scrapy的shell环境中。在这个环境中，你可以跟在爬虫的`parse`方法中一样使用了。

