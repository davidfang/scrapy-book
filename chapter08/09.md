## <center>pipelines的使用</center>

> 前面介绍了我们使用`scrapy`框架爬取数据的时候,要提取到数据库或者写入本地文件中,我们正常的方式会先定义`Item`约束需要下载的字段,然后再`Spider`中使用,当`Spider`下载的数据,会交给`pipelines`中处理,写入到本地文件中或者数据库中(`pipelines`更多的功能主要是写入数据)

### 一、关于`pipelines`中主要的方法

* 1、`process_item(self, item, spider):`必须写的方法
  * 1.`Item`:`Spider`爬虫中`yield`出来的`item`对象
  * 2.`spider`:是`Spider`对象,如果一个工程中有多个爬虫就需要使用`spider.name`来判断当前的爬虫

* 2、`open_spider(self, spider):`代表开启爬虫的时候执行的方法(常用于开启数据库连接,开启文件写入)
* 3、`close_spider(self, spider):`代表关闭爬虫的时候执行的方法(常用于关闭数据库,关闭文件写入)
* 4、`from_crawler(cls, crawler):`定义一个类方法(主要使用`Scrapy`的核心组件,比如`settings`、`singals`)

### 二、使用`pipelines`将数据写入`mongodb`中

* 1、在`settings.py`中设置`mongodb`数据库连接

  ```py
  # 配置mongodb数据库
  MONGO_URI = 'localhost'
  MONGO_DATABASE = 'py_test'
  ```

* 2、书写`pipelines`

  ```py
  import pymongo

  class MongoPipeline(object):
      # 定义表名
      collection_name = 'jobb_item'

      def __init__(self, mongo_url, mongo_db):
          self.mongo_url = mongo_url
          self.mongo_db = mongo_db
          self.client = None
          self.db = None

      @classmethod
      def from_crawler(cls, crawler):
          return cls(
              mongo_url=crawler.settings.get('MONGO_URI'),
              mongo_db=crawler.settings.get('MONGO_DATABASE', 'item')
          )

      def open_spider(self, spider):
          """
          打开爬虫的时候
          :param spider:
          :return:
          """
          self.client = pymongo.MongoClient(self.mongo_url)
          self.db = self.client[self.mongo_db]

      def close_spider(self, spider):
          """
          爬虫关闭的时候
          :param spider:
          :return:
          """
          self.client.close()

      def process_item(self, item, spider):
          """
          主要处理数据
          :param item:
          :param spider:
          :return:
          """
          if spider.name == 'blog':
              self.db[self.collection_name].insert(dict(item))
              return item
  ```

* 3、在`settings.py`中注册自己写的`pipelines`类

  ```py
  ITEM_PIPELINES = {
      'jobbloe.pipelines.MongoPipeline': 300,
  }
  ```
