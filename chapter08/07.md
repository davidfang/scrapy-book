### <center>Item和ItemLoader的使用</center>

> 本章节中要[抓取的网站](http://quotes.toscrape.com/)

### 一、爬虫的目的

我们写爬虫主要目的就是从网页中把哪些非结构性的数据提取出结构性数据,存储到数据库中,在`spider`的类中默认会创建`parse()`方法,该方法就是用来解析网页上数据源,`Item`就是将这些提取的数据**包装为结构化数据**


`Item`对象是一种简单的容器,用来保存爬取到的数据,提供类似于字典的`API`以及用来声明可用字段的简单语法.


`Item`的使用一般分为两种:

* 1、常规的`Item`(就是定义普通需要下载的字段)
* 2、`ItemLoader`的使用

### 二、关于`Item`与`ItemLoader`的区别
`ItemLoader`是负责数据的收集、处理、填充,`item`仅仅是承载了数据本身, `Items`提供了抓取数据的容器，而`Item Loaders`提供了填充该容器的机制。
数据的收集、处理、填充归功于`item loader`中两个重要组件:
* 输入处理`input processors`
* 输出处理`output processors`

### 三、`Item`的使用

* 1、定义一个`Item`的类

  ```py
  import scrapy

  class QuotesPageItem(scrapy.Item):
      content = scrapy.Field()
      tags = scrapy.Field()
  ```

* 2、在`spider`爬虫中使用定义好的`Item`

  ```py
  import scrapy

  from quotes_page.items import QuotesPageItem

  class QuotesSpider(scrapy.Spider):
      name = 'quotes'
      allowed_domains = ['quotes.toscrape.com']
      start_urls = ['http://quotes.toscrape.com/']

      def parse(self, response):
          quote_list = response.xpath('//div[@class="quote"]')
          for quote in quote_list:
              content = quote.xpath('./span[@class="text"]/text()').get()
              tag = quote.xpath('.//a[@class="tag"]/text()').getall()
              tags = ','.join(tag)
              # 直接使用刚刚定义的`Item`
              yield QuotesPageItem(content=content, tags=tags)
  ```

* 3、运行命令运行爬虫

  ```py
  scrapy crawl quotes --nolog -o quotes.json
  ```

### 四、`ItemLoader`的使用,[官方文档](https://doc.scrapy.org/en/latest/topics/loaders.html)
