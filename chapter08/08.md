## <center>spider的理解</center>

### 一、使用命令创建新爬虫

* 1、创建命令

  ```py
  scrapy genapider 爬虫名称 网址
  ```

* 2、在`spiders`包中会生成一只爬虫文件

  ```py
  # -*- coding: utf-8 -*-
  import scrapy

  class QuotesSpider(scrapy.Spider):
      name = 'quotes' # 爬虫名字
      allowed_domains = ['quotes.toscrape.com'] # 允许访问的域
      start_urls = ['http://quotes.toscrape.com/']  # 起始的url地址

      def parse(self, response):
          """
          爬虫访问起始的url地址后的回调函数
          主要有两个作用:第一个是提取想要的数据,第二个是生成下一个请求的url。
          """
          pass
  ```

### 二、关于`url`地址的拼接问题
* 1、可以直接使用传统的字符串拼接方式(`format`及`%d`的方式)
* 2、使用`urllib`模块(推荐使用)

  ```py
  from urllib import parse

  if __name__ == "__main__":
      print(parse.urljoin('https://www.baidu.com', 'https://www.baidu.com'))
      print(parse.urljoin('https://www.baidu.com', '10'))

  # 输出结果
  # https://www.baidu.com
  # https://www.baidu.com/10
  ```

### 三、使用`meta`在不同的请求中传递参数

* 1、在`Request()`中直接使用`meta`传递参数到下一个请求中
* 2、在下一个请求中使用`response.meta.get(字段名, '')`获取上一个请传递过来的参数

### 四、在爬虫中生成一个`url`请求(连续性爬虫)

* 1、使用`Request`需要导包

  ```py
  from scrapy.http import Request
  ```

* 2、关于`Request()`源码

  ```py
  class Request(object_ref):

      def __init__(self, url, callback=None, method='GET', headers=None, body=None,
                   cookies=None, meta=None, encoding='utf-8', priority=0,
                   dont_filter=False, errback=None, flags=None):
          """
          url:需要请求的url地址
          callback:请求完url后需要执行的回调函数
          method:请求方式
          headers:请求头
          body:请求体
          cookies:cookies
          meta:主要作用是在多个请求之间传递参数
          """
  ```

* 3、使用案例[爬虫伯乐在线](http://python.jobbole.com/all-posts/)

  ```py
  import scrapy
  from scrapy.http import Request
  from urllib import parse

  class BlogSpider(scrapy.Spider):
      name = 'blog'
      allowed_domains = ['blog.jobbole.com', 'python.jobbole.com']
      start_urls = ['http://python.jobbole.com/all-posts/']
      # allowed_domains = ['blog.jobbole.com']  
      # start_urls = ['http://blog.jobbole.com/all-posts/']

      def parse(self, response):
          url_lists = response.xpath('//div[@id="archive"]/div[@class="post floated-thumb"]')

          for current_url in url_lists:
              detail_link = current_url.xpath('./div[@class="post-meta"]//a/@href').extract_first()
              title = current_url.xpath('./div[@class="post-meta"]//a[@class="archive-title"]/text()').extract_first()
              # 重新请求,将title传递到下一个url中
              yield Request(url=parse.urljoin(response.url, detail_link), callback=self.detail_parse,
                            meta={'title': title})

      def detail_parse(self, response):
          """
          获取详情页面数据
          :return:
          """
          print('---当前url地址----', response.url)
          info = response.xpath('//div[@class="entry-meta"]/p[@class="entry-meta-hide-on-mobile"]/text()').get().strip()
          # 获取上一个url传递过来meta信息
          title = response.meta.get('title', '')
          print(title, '==>', info)
  ```
