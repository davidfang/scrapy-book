## <center>scrapy的安装</center>

### 一、在`window`系统下安装

* 1、进入工作空间
* 2、尝试使用最简单的命令安装

  ```py
  pip3 install scrapy
  # 报错信息如下
  # Failed building wheel for Twisted
  # Microsoft Visual C++ 14.0 is required...
  ```

* 3、工作空间先安装`whl`

  ```py
  pip3 install whl
  ```

* 4、到[网站下载](https://www.lfd.uci.edu/~gohlke/pythonlibs/)
  * 1.`scrapy`依赖`twiste`先查找`twisted`下载
  * 2.查找`scrapy`下载

* 5、安装`tristed`

  ```py
  pip3 install Twisted-17.1.0-cp36-cp36m-win_amd64.whl
  ```

* 6、安装`scrapy`

  ```py
  pip3 install Scrapy-1.3.3-py2.py3-none-any.whl
  ```

* 7、测试

  ```py
  (scrapy_page) ➜  ~ scrapy -h
  ```

### 二、在`mac`系统下安装

* 1、进入工作空间
* 2、使用`pip3`安装

  ```py
  pip3 install scrapy
  ```

### 三、`scrapy`爬虫初探

* 1、创建一个`scrapy`项目

  ```py
  scrapy startproject tutorial[项目名称]
  ```

* 2、关于`scrapy`项目文件介绍

  ```py
  |____scrapy.cfg     # Scrapy部署时的配置文件
  |____tutorial         # 项目的模块，引入的时候需要从这里引入
  | |______init__.py    
  | |______pycache__
  | |____items.py     # Items的定义，定义爬取的数据结构
  | |____middlewares.py   # Middlewares的定义，定义爬取时的中间件
  | |____pipelines.py       # Pipelines的定义，定义数据管道
  | |____settings.py       # 配置文件
  | |____spiders         # 放置Spiders的文件夹
  | | |______init__.py
  | | |______pycache__
  ```

* 3、创建一个爬虫

  ```py
  cd tutorial
  scrapy genspider quotes(爬虫名字,一个工程中唯一的) "网址"
  ```

* 4、会自动在`spiders`包下生成一个以爬虫名称为名称的文件

  ```py
  # -*- coding: utf-8 -*-
  import scrapy

  class QuotesSpider(scrapy.Spider):
      name = "quotes"
      allowed_domains = ["quotes.toscrape.com"]
      start_urls = ['http://quotes.toscrape.com/']

      def parse(self, response):
          pass
  ```

* 5、书写`item`

  ```py
  import scrapy

  class QuoteItem(scrapy.Item):

      text = scrapy.Field()
      author = scrapy.Field()
      tags = scrapy.Field()
  ```

* 6、将`item`关联到爬虫中

  ```py
  from scrapytest.items import QuoteItem

  class QuotesSpider(scrapy.Spider):
      name = 'quotes'
      allowed_domains = ['quotes.toscrape.com']
      start_urls = ['http://quotes.toscrape.com/']

      def parse(self, response):
          quotes = response.css('.quote')
          for quote in quotes:
              item = QuoteItem()
              item['text'] = quote.css('.text::text').extract_first()
              item['author'] = quote.css('.author::text').extract_first()
              item['tags'] = quote.css('.tags .tag::text').extract()
              yield item
  ```

* 7、运行爬虫

  ```py
  scrapy crawl quotes # 直接运行
  scrapy crawl quotes -o quotes.json # 生成json文件存到本地
  ```
