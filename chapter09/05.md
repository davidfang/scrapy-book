## <center>关于scrapy中url去除原理二</center>

> 前面一章节介绍了将已经访问过的request添加到set集合及本地文件中,本章节主要介绍将访问过的request存储到redis及介绍scrapy-redis的存储原理

### 一、存储到`redis`中,具体代码就不解释了

```py
from scrapy.dupefilters import BaseDupeFilter
from scrapy.utils.request import request_fingerprint
import redis

class BlogDupeFilter(BaseDupeFilter):
    """
    自定义一个url去重的类
    """

    def __init__(self):
        self.conn = redis.Redis(host='', port=6379, password='')

    @classmethod
    def from_settings(cls, settings):
        return cls()

    def request_seen(self, request):
        print('当前的request', request)
        # 如果已经存在就会返回0
        result = self.self.conn.sadd(request_fingerprint(request))
        if result == 1:
            return False
        return True
```

### 二、使用`scrapy-redis`的存储

* 1、需要安装包

  ```py
  pip3 install scrapy-redis
  ```

* 2、查看源码(点击进去)

  ```py
  from scrapy_redis.dupefilter import RFPDupeFilter
  ```

* 3、直接在`settings.py`中配置就可以使用`scrapy-redis`的去重及存储功能

  * 1、指定使用`scrapy-redis`的`url`去重

    ```py
    # 配置url去重的原理
    # from scrapy_redis.dupefilter import RFPDupeFilter
    DUPEFILTER_CLASS = "scrapy_redis.dupefilter.RFPDupeFilter"
    ```

  * 2、配置`redis`的信息

    ```py
    REDIS_HOST = '10.211.55.13'                           # 主机名
    REDIS_PORT = 6379                                     # 端口
    # REDIS_URL = 'redis://user:pass@hostname:6379'       # 连接URL（优先于以上配置）
    # REDIS_PARAMS  = {}                                  # Redis连接参数             默认：REDIS_PARAMS = {'socket_timeout': 30,'socket_connect_timeout': 30,'retry_on_timeout': True,'encoding': REDIS_ENCODING,}）
    # REDIS_PARAMS['redis_cls'] = 'myproject.RedisClient' # 指定连接Redis的Python模块  默认：redis.StrictRedis
    REDIS_ENCODING = "utf-8"                              # redis编码类型             默认：'utf-8'
    ```
